# ğŸ“Š Generic Data Ingestion Framework - FYP Version

A simplified Python framework for ingesting and processing JSON data files with automatic schema inference and SQLite database storage. Designed for academic demonstration and final year project requirements.

## ğŸ¯ Project Overview

This is an educational implementation of a data ingestion framework that demonstrates core concepts of data processing, schema inference, and database integration. It focuses on simplicity and clear demonstration of fundamental principles.

### Key Features (Simplified)

- ğŸ“ JSON file processing and validation
- ğŸ” Automatic schema inference from data
- ğŸ’¾ SQLite database storage
- ğŸŒ Simple web interface using Streamlit
- âŒ¨ï¸ Command-line interface for automation
- ğŸ“Š Basic error handling and logging

### Academic Focus Areas

This project demonstrates understanding of:
- File I/O operations and data processing
- JSON parsing and validation
- Database design and integration
- Schema inference algorithms  
- Web interface development
- Error handling strategies
- Modular software architecture

## ğŸ—ï¸ Simplified Architecture

```
src/
â”œâ”€â”€ core/
â”‚   â””â”€â”€ application.py          # Main application logic
â”œâ”€â”€ connectors/
â”‚   â”œâ”€â”€ database_connector.py   # Base database interface
â”‚   â”œâ”€â”€ sqlite_connector.py     # SQLite implementation
â”‚   â””â”€â”€ connector_factory.py    # Simple factory pattern
â”œâ”€â”€ processors/
â”‚   â”œâ”€â”€ json_processor.py       # JSON data processing
â”‚   â””â”€â”€ schema_processor.py     # Schema inference
â”œâ”€â”€ scanners/
â”‚   â””â”€â”€ file_scanner.py         # File discovery
â””â”€â”€ handlers/
    â”œâ”€â”€ file_handler.py         # File operations
    â”œâ”€â”€ error_handler.py        # Error management
    â””â”€â”€ logging_handler.py      # Logging utilities
```

### Design Patterns Demonstrated

- **Factory Pattern**: Database connector creation
- **Strategy Pattern**: Data processing approaches
- **Modular Architecture**: Clear separation of concerns
- **Error Handling**: Graceful failure management

## ğŸš€ Getting Started

### Prerequisites

- Python 3.8 or higher
- Basic understanding of JSON data format
- SQLite (included with Python)

### Installation

1. Clone the repository:
   ```bash
   git clone <repository-url>
   cd generic_data_ingestor_framework
   ```

2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

### Usage Options

#### Option 1: Web Interface (Recommended)

Launch the simplified Streamlit interface:

```bash
streamlit run app.py
```

**Web Interface Features:**
- ğŸ“¤ File upload with drag-and-drop
- ğŸ” Automatic data analysis 
- ğŸ’¾ SQLite database storage
- ğŸ“Š Results preview and download
- âœ… Processing status indicators

#### Option 2: Command Line

Use the simple CLI for batch processing:

```bash
# Basic usage
python main.py data/

# Specify output database and table
python main.py data/ --output mydata.db --table customers

# Quiet mode (minimal output)
python main.py data/ --quiet
```

**CLI Options:**
- `directory`: Path to folder containing JSON files
- `--output, -o`: SQLite database file name (default: output.db)
- `--table, -t`: Table name (default: processed_data)
- `--quiet, -q`: Suppress informational messages

## ğŸ“‹ Example Workflow

### 1. Prepare Sample Data

Create some JSON files in a `data/` directory:

**data/customers.json**
```json
[
  {"id": 1, "name": "John Doe", "email": "john@example.com", "age": 30},
  {"id": 2, "name": "Jane Smith", "email": "jane@example.com", "age": 25}
]
```

**data/products.json**
```json
[
  {"product_id": "P001", "name": "Laptop", "price": 999.99, "in_stock": true},
  {"product_id": "P002", "name": "Mouse", "price": 29.99, "in_stock": false}
]
```

### 2. Process with Web Interface

1. Run `streamlit run app.py`
2. Upload your JSON files
3. Click "Process Files"
4. View results and download processed data

### 3. Process with CLI

```bash
python main.py data/ --output sales_data.db --table all_data
```

### 4. Examine Results

The processed data will be stored in SQLite with:
- Inferred column types
- Source file metadata
- Processing timestamps

## ğŸ“ Academic Learning Outcomes

This project demonstrates proficiency in:

### Technical Skills
- **Python Programming**: Object-oriented design, error handling, file I/O
- **Data Processing**: JSON parsing, validation, transformation
- **Database Integration**: Schema design, SQL operations, data loading
- **Web Development**: Streamlit interface creation
- **Software Architecture**: Modular design, design patterns

### Analytical Skills
- **Schema Inference**: Automatic detection of data types and structures
- **Error Handling**: Graceful handling of malformed data
- **Performance Considerations**: Efficient processing of multiple files

### Project Management
- **Documentation**: Clear code documentation and user guides
- **Testing**: Basic validation and error scenarios
- **Version Control**: Structured codebase organization

## ğŸ“Š Sample Output

After processing, you'll see results like:

```
ğŸš€ Generic Data Ingestion Framework - FYP Version
ğŸ“ Processing directory: data/
ğŸ—„ï¸  Output database: output.db
ğŸ“Š Table name: processed_data

Found 2 JSON files to process
Processing: customers.json
  âœ“ Processed 2 records
Processing: products.json  
  âœ“ Processed 2 records
Saving 4 records to database: output.db
Creating table: processed_data

âœ… Processing completed successfully!
ğŸ“ˆ Summary:
  â€¢ Files processed: 2/2
  â€¢ Records saved: 4
  â€¢ Processing time: 0.15s
  â€¢ Database: output.db
  â€¢ Table: processed_data
```

## ğŸ”® Future Enhancements

The current simplified version provides a solid foundation for potential improvements:

### Phase 1: Enhanced Features
- Support for CSV and XML file formats
- Advanced data validation rules
- Improved error reporting and recovery
- Configuration file support

### Phase 2: Production Features  
- Multiple database support (PostgreSQL, MySQL)
- Connection pooling and optimization
- Real-time processing capabilities
- Advanced web interface with authentication

### Phase 3: Enterprise Scale
- Distributed processing support
- Cloud deployment options
- API development for external integration
- Machine learning-based data quality assessment

*See [FUTURE_ADVANCEMENTS.md](FUTURE_ADVANCEMENTS.md) for detailed roadmap.*

## ğŸ§ª Testing

Basic testing can be performed with:

```bash
# Test with sample data
python main.py test_data/

# Verify database contents
sqlite3 output.db "SELECT * FROM processed_data LIMIT 5;"
```

## ğŸ“š Project Structure

```
generic_data_ingestor_framework/
â”œâ”€â”€ README.md                        # This file
â”œâ”€â”€ FUTURE_ADVANCEMENTS.md           # Detailed enhancement roadmap
â”œâ”€â”€ requirements.txt                 # Python dependencies
â”œâ”€â”€ app.py                           # Streamlit web interface  
â”œâ”€â”€ main.py                          # Command-line interface
â”œâ”€â”€ src/                            # Source code
â”‚   â”œâ”€â”€ core/application.py         # Main application (186 lines)
â”‚   â”œâ”€â”€ connectors/                 # Database layer (4 files)
â”‚   â”œâ”€â”€ processors/                 # Data processing (2 files)
â”‚   â”œâ”€â”€ scanners/                   # File discovery (1 file)
â”‚   â””â”€â”€ handlers/                   # Utilities (3 files)
â””â”€â”€ test_data/                      # Sample data for testing
```

## âœ… Requirements Fulfilled

This implementation satisfies the FYP requirements:

- âœ… **FR1**: JSON file ingestion and processing
- âœ… **FR2**: Data validation and format checking  
- âœ… **FR3**: Recursive directory traversal
- âœ… **FR4**: Filtering and error handling
- âœ… **FR5**: Metadata extraction and logging
- âœ… **FR6**: Configurable processing rules
- âœ… **FR7**: Consistent output generation
- âœ… **FR8**: Extensible architecture for new formats

- âœ… **NFR1**: Modular design with clear separation
- âœ… **NFR2**: Extensible system architecture  
- âœ… **NFR3**: Efficient processing performance
- âœ… **NFR4**: Comprehensive error logging
- âœ… **NFR5**: Robust error handling
- âœ… **NFR6**: Clear code documentation

## ğŸ‰ Conclusion

This simplified Generic Data Ingestion Framework demonstrates core data processing concepts while maintaining clean, understandable code suitable for academic evaluation. The architecture provides a solid foundation for understanding enterprise data processing systems while remaining accessible for educational purposes.

The project successfully balances academic requirements with real-world applicability, providing both immediate functionality and a clear path for future enhancement into production-ready systems.

---

*This is the simplified FYP version. For the full-featured production roadmap, see [FUTURE_ADVANCEMENTS.md](FUTURE_ADVANCEMENTS.md).*